# ðŸ“¦ Open Gym Case Studies â€” Logistics & Robotics  

> _Textbook-aligned case studies for teaching Q-Learning / SARSA and extending to DQN / Actorâ€“Critic._

[![CI](https://github.com/srikanthbaride/open-gym-case-studies/actions/workflows/ci.yml/badge.svg)](https://github.com/srikanthbaride/open-gym-case-studies/actions/workflows/ci.yml)
[![Last Commit](https://img.shields.io/github/last-commit/srikanthbaride/open-gym-case-studies)](https://github.com/srikanthbaride/open-gym-case-studies/commits/main)
![Textbook Alignment](https://img.shields.io/badge/Aligned_with-Reinforcement_Learning_Explained-blue)
[![CRC Press 2025](https://img.shields.io/badge/CRC%20Press-2025-blue)](https://www.routledge.com/)
![Part III â€“ Case Studies](https://img.shields.io/badge/Part%20III-Case%20Studies-informational)
[![License](https://img.shields.io/badge/License-Educational-lightgrey.svg)](#license)

This repository contains **two real-world-motivated case studies** implemented with **Gymnasium (OpenAI Gym)**, aligned with the reinforcement-learning chapters (Bandits â†’ MC â†’ TD â†’ Q-Learning/SARSA) in the textbook  
ðŸ“˜ **[_Reinforcement Learning Explained_ (CRC Press | Taylor & Francis, 2025)](https://github.com/srikanthbaride/rl-explained-preprint)**.

Each case study includes **reproducible training scripts**, **CSV logs**, **plots**, and **pytest smoke tests**.  
Extensions are scaffolded for **Deep RL (DQN / Actorâ€“Critic)**.

---

## ðŸ“˜ Relation to the Textbook

| Chapter | Environment | Description | Folder |
|:--|:--|:--|:--|
| **Ch. 12 â€“ Interacting with Environments using Gymnasium** | `gymnasium` API | Unified interface for observation/action spaces and episode control | `case_studies/00_frozenlake_navigation/` |
| **Ch. 13 â€“ Taxi-v3 : Temporal-Difference Control in a Discrete Grid World** | `Taxi-v3` | Q-Learning vs SARSA with Îµ-decay schedules and Bellman-optimal policy convergence | `case_studies/01_taxi_last_mile/` |
| **Ch. 14 â€“ LunarLander-v3 : Continuous-State Control and the Curse of Dimensionality** | `LunarLander-v3` | Discretization & state aggregation bridging toward Deep RL | `case_studies/02_lunar_lander_drone/` |

---

## Case Studies

1. **Last-Mile Dispatch (Taxi-v3)**  
   _Real-world lens_: courier / ride-hailing pickupâ€“drop logistics in a small grid.  
   _Env_: `gymnasium.envs.toy_text.taxi.TaxiEnv` (discrete).  
   _Algos_: Q-Learning, SARSA (Îµ-greedy, decaying Îµ).  
   _Metrics_: average return vs episodes, success rate, steps per episode.

2. **Drone Landing Guidance (LunarLander-v3)**  
   _Real-world lens_: autonomous drone landing under stochastic dynamics.  
   _Env_: `LunarLander-v3` (Box2D, discrete).  
   _Algos_: Q-Learning, SARSA (tabular baseline) + DQN scaffold.  
   _Metrics_: average return vs episodes, solved rate (â‰¥ 200), crash rate.

> **Why these?** They map cleanly to real operations (dispatch & landing control) and trace a pedagogical line from **MDPs & Bellman** â†’ **TD Control** â†’ **Function Approximation (Deep RL)**.

---

## âš™ï¸ Quickstart

```bash
# Python 3.10 + recommended
pip install -r requirements.txt

# Case Study 1: Taxi (Last-Mile Dispatch)
python case_studies/01_taxi_last_mile/train_q_learning.py --episodes 4000
python case_studies/01_taxi_last_mile/plot_returns.py

# Case Study 2: Lunar Lander (Drone Landing)
python case_studies/02_lunar_lander_drone/train_q_learning.py --episodes 5000
python case_studies/02_lunar_lander_drone/plot_returns.py
```

Artifacts appear under each studyâ€™s `runs/` folder (`.csv`, `.npy`, plots).  
â± Typical runtime: ~5 min @ CPU for Taxi-v3, ~10 min for LunarLander-v3.

---

## ðŸ“‚ Repo Layout

```
open-gym-case-studies/
â”œâ”€ README.md
â”œâ”€ requirements.txt
â”œâ”€ Makefile
â”œâ”€ common/
â”‚  â”œâ”€ utils.py
â”‚  â””â”€ plotting.py
â”œâ”€ case_studies/
â”‚  â”œâ”€ 00_frozenlake_navigation/  # Tabular SARSA & Q-Learning
â”‚  â”œâ”€ 01_taxi_last_mile/         # Q-Learning, SARSA, evaluation + plots
â”‚  â””â”€ 02_lunar_lander_drone/     # Tabular baselines + DQN scaffold
â”œâ”€ tests/                         # pytest smoke tests
â””â”€ .github/workflows/ci.yml       # CI with caching + artifacts
```

---

## ðŸ§© Deep RL â€” Expansion Path

- **DQN (discrete control)** â†’ replace Q-table with neural Q-network, add replay buffer & target network.  
- **Stability extensions:** Double DQN, prioritized replay.  
- **Policy Gradients:** Actorâ€“Critic / A2C / PPO.  
- **Continuous control:** `LunarLanderContinuous-v2` with DDPG / TD3 / SAC.

> Optional: `pip install torch` to activate DQN scaffold in `02_lunar_lander_drone/dqn_scaffold.py`.

---

## ðŸ§ª Reproducibility & Evaluation

- Deterministic seeding (`--seed`, NumPy + Gymnasium).  
- CSV logs â†’ `episode, return, epsilon, steps, success`.  
- Plots â†’ moving average return with 95 % CI (standard error).  
- Pytest smoke tests ensure training-loop integrity.

---

## ðŸ‘©â€ðŸ’» Contributing (for RAs / students)

See [`CONTRIBUTING.md`](CONTRIBUTING.md) and [`CODE_OF_CONDUCT.md`](CODE_OF_CONDUCT.md).  
Open PRs with small, reviewable chunks â€” CI runs on every PR.

---

## ðŸ“š Citation

If you use this code or the accompanying book in research or teaching, please cite:

**Book (forthcoming):**
```bibtex
@book{baride2025rlexplained,
  author    = {Srikanth Baride and Rodrigue Rizk and KC Santosh},
  title     = {Reinforcement Learning Explained},
  publisher = {CRC Press | Taylor \& Francis Group},
  year      = {2025},
  isbn      = {9781041252993},
  note      = {Accepted for publication; preprint available at \url{https://github.com/srikanthbaride/rl-explained-preprint}}
}
```

---

## ðŸªª License

Educational use; adapt as needed for your textbookâ€™s distribution terms.

---

## Â© Copyright & Attribution

Â© 2025 Dr. Srikanth Baride, Dr. Rodrigue Rizk, and Prof. KC Santosh.  
All rights reserved. This repository accompanies the textbook:

> **_Reinforcement Learning Explained_**  
> CRC Press / Taylor & Francis Group, 2025.

The source code and instructional content were developed by the authors for educational and research purposes.  
Algorithms (Q-Learning, SARSA, DQN scaffolds) are implemented from scratch following established formulations.

This project uses **Gymnasium (OpenAI Gym)** under the MIT License; no third-party proprietary code is included.

**Permitted use:** Educators and students may reproduce, modify, and distribute this material for non-commercial educational purposes, with proper credit and citation.

For permissions beyond classroom use, contact the authors via the University of South Dakota AI Research Lab.
